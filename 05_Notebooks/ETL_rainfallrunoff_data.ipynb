{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook voor het omzetten van de met Wageningenmodellen en overstortschatter berekende afvoeren naar lateralen D_HYDRO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import splitext, join, abspath, basename\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as et \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set relative paths to input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute path of the script, which will serve as a base to obtain relative paths to all other used files.\n",
    "path_script = os.path.abspath('')\n",
    "path_data = join(os.path.dirname(path_script), r'00_Data')\n",
    "\n",
    "# Shapefiles\n",
    "laterals_shp_path = join(path_data, r'input_afvoeren\\shapefiles\\laterals.shp')\n",
    "wagmod_shp_path = join(path_data, r'input_afvoeren\\shapefiles\\NAvakken.shp')\n",
    "\n",
    "overstorten_shp_path = join(path_data, r'input_afvoeren\\\\shapefiles\\lozingspunten.shp')\n",
    "modelarea_shp_path = join(path_data, r'input_afvoeren\\shapefiles\\HoogeRaam.shp')\n",
    "\n",
    "# Timeseries\n",
    "laterals_data_path = join(path_data, r'input_afvoeren\\timeseries\\landelijk')\n",
    "\n",
    "overstorten_data_path = join(path_data, r'input_afvoeren\\timeseries\\overstorten')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGRITY = True # If true, checks if indexes of dataframes are unique.\n",
    "SAVE_INTERMEDIATE_RESULTS = True # If true, saves outputs of scenarios to .csv files to check validity.\n",
    "SCENARIO = 'hist' # pick one scenario ('hist', 'ghg120', etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the shapefiles \n",
    "- The locations of the laterals to which the must finally be coupled\n",
    "- The location of the overstorten\n",
    "- The polygons that are used by the Wageningen Model, to know which discharge to couple with which laterals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_modelarea = gpd.read_file(modelarea_shp_path)\n",
    "\n",
    "gdf_laterals = (\n",
    "    gpd.read_file(laterals_shp_path)\n",
    "    .set_index('CODE', verify_integrity = True)\n",
    "    [['Opp_hactar','X','Y','geometry']]\n",
    ")\n",
    "gdf_laterals['type'] = 'laterals'\n",
    "gdf_laterals = gdf_laterals[gdf_laterals.within(gdf_modelarea.at[0,'geometry'])]\n",
    "\n",
    "gdf_wagmod = (\n",
    "    gpd.read_file(wagmod_shp_path)\n",
    "    [['GAFIDEN','geometry']]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gdf_overstorten = gpd.read_file(overstorten_shp_path).set_index('RIODAT_ZRO', \n",
    "    verify_integrity=True \n",
    "    )[['geometry']]\n",
    "gdf_overstorten['type'] = 'overstort'\n",
    "gdf_overstorten = gdf_overstorten[gdf_overstorten.within(gdf_modelarea.at[0,'geometry'])]\n",
    "\n",
    "gdf_laterals.head()\n",
    "gdf_wagmod.head()\n",
    "gdf_overstorten.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the tables of the laterals and the overstorten and save this shapefile. \n",
    "One output of the script is to create a shapefile with all the locations to which model data must be coupled, which is hereby done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the shapefile containing all locations (laterals and overstorten). This is a component of the final output.\n",
    "pd.concat([gdf_laterals,gdf_overstorten], \n",
    "    verify_integrity=False \n",
    "    )[['geometry','type']].to_file(join(path_data, r'output_afvoeren\\shapefiles\\lateralen_locations'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wageningen Model Polygons contain values for GAFIDEN, which is an identifier used to obtain Specifieke Afvoer. Most laterals intersect a polygon, but for the 108 that do not, the join is made based on nearest polygon; a column is added containing the distance from the lateral to the nearest polygon. A new dataframe (gdf_unexactmatches) is created that contains laterals that are more than zero away from a polygon, just to easily see which ones aren't matched directly.\n",
    "\n",
    "Unfortunately, the current delft3dfmpy environment (2.0.3) uses a fairly outdated version of geopandas. Therefore, rather than the geodataframe.sjoin_nearest() function, I am forced to use a for loop with a runtime of 30-40 seconds. This can be changed when the environment updates geopandas to a more recent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=gpd.GeoDataFrame(gdf_wagmod.geometry)\n",
    "gdf_laterals_joined = gdf_laterals\n",
    "\n",
    "for point in gdf_laterals_joined.geometry.index:\n",
    "    nearest_polygon = polygons.distance(gdf_laterals_joined.geometry[point]).sort_values().index[0]\n",
    "    gdf_laterals_joined.loc[point, 'GAFIDEN'] = gdf_wagmod.loc[nearest_polygon,'GAFIDEN']\n",
    "\n",
    "    distance = polygons.distance(gdf_laterals_joined.geometry[point]).min()\n",
    "    gdf_laterals_joined.loc[point, 'afstand'] = distance\n",
    "    \n",
    "gdf_laterals_joined.head()\n",
    "gdf_unexactmatches = gdf_laterals_joined[gdf_laterals_joined['afstand']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the different shapes to create an idea of what we are dealing with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plotted the laterals to give an idea of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(10, 10))\n",
    "ax1.set_xlim(170000, 200000)\n",
    "ax1.set_ylim(390000, 420000)\n",
    "ax2.set_xlim(185000, 195000)\n",
    "ax2.set_ylim(410000, 420000)\n",
    "\n",
    "for plot in (ax1, ax2):\n",
    "    \n",
    "    fig=gdf_wagmod.plot(ax=plot, fc=\"lightyellow\", edgecolor='black', lw=0.7)\n",
    "    fig = gdf_modelarea.plot(ax=plot, fc='none', edgecolor='black', lw=2)\n",
    "    fig=gdf_overstorten.plot(ax=plot, color = 'purple', marker='x',markersize = 5, label='Overstorten')\n",
    "    fig=gdf_laterals_joined[gdf_laterals_joined['afstand']==0].plot(ax=plot, color='green', label = 'Directe match', markersize = 5)\n",
    "    fig=gdf_laterals_joined[gdf_laterals_joined['afstand']>0].plot(ax=plot, color='red', label = 'Match door nabijheid', markersize = 5)\n",
    "    plot.legend()\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform and Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landelijke afvoer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfolder = join(path_data,r'input_afvoeren/timeseries/landelijk/')\n",
    "\n",
    "wagmodFiles = os.listdir(join(inputfolder, SCENARIO))\n",
    "\n",
    "## Read Wageningen model output (discharges) per region, and multiply by the area belonging to each lateral\n",
    "ls_wagmodoutput = []\n",
    "\n",
    "for region in wagmodFiles:\n",
    "    \n",
    "    metadata_region = pd.read_csv(join(inputfolder, SCENARIO, region), nrows=1, skiprows=[0,2], header=None)[0][0]\n",
    "    model = re.search('model:(.*) (?:van|met winterbui)', metadata_region).group(1)\n",
    "            \n",
    "    df_region = pd.read_csv(\n",
    "            join(inputfolder, SCENARIO, region), \n",
    "            header=4, \n",
    "            delim_whitespace=True, \n",
    "            usecols=['-I-','-QC-'],\n",
    "            )\n",
    "\n",
    "    df_region=df_region.set_index(\n",
    "            pd.to_datetime(df_region['-I-'],\n",
    "            origin=pd.Timestamp('2010-11-09 00:00:00'),\n",
    "            unit='h')\n",
    "        ).drop(labels=['-I-'], axis='columns').rename(columns={'-QC-':model})\n",
    "    \n",
    "    ls_wagmodoutput.append(df_region)\n",
    "\n",
    "# The result is a list containing dataframes as items\n",
    "ls_wagmodoutput, len(ls_wagmodoutput)\n",
    "\n",
    "# Combine the dataframes into one big dataframe\n",
    "df_wagmodoutput = pd.concat(ls_wagmodoutput, axis=1, verify_integrity=True).T * 10 / 3600\n",
    "\n",
    "# Set the index to be only the 'GAFIDEN'\n",
    "for i in range(len(df_wagmodoutput.index)):\n",
    "    ID = df_wagmodoutput.index[i].split(' ')[0]\n",
    "    df_wagmodoutput.loc[df_wagmodoutput.index[i],'GAFIDEN'] = ID\n",
    "\n",
    "### Multiplication\n",
    "gdf_laterals_joined['Latid'] = gdf_laterals_joined.index\n",
    "output_laterals = gdf_laterals_joined.merge(df_wagmodoutput, on='GAFIDEN', how='left')\n",
    "output_laterals.set_index('Latid', inplace=True)\n",
    "output_landelijkeafvoer = output_laterals.iloc[:,8:].multiply(output_laterals['Opp_hactar'],axis='index').T\n",
    "output_landelijkeafvoer.index = pd.to_datetime(output_landelijkeafvoer.index)\n",
    "\n",
    "if SAVE_INTERMEDIATE_RESULTS == True:\n",
    "    output_landelijkeafvoer.to_csv(join(path_data, r'output_afvoeren\\timeseries\\landelijk', SCENARIO + '.csv'), sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overstorten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions\n",
    "Reading XML files can be a hassle and it doesn't improve the readability of the code. Therefore I wrapped it into a function that takes the path of an xml file and produces a list that contains a dataframe for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(path_xml):\n",
    "    \"\"\"XML files consist of a root node that can have numerous generation of childnodes. This function takes the path to an xml from a certain year and \n",
    "    returns a list of first generation nodes, which I called series (same as in the xml files). Each Serie belongs to a specific overstort, and contain the second generation of childnodes: \n",
    "    - one Header (metadata) \n",
    "    - many Events (the actual data).\n",
    "    The function returns a list of which each item is a serie, which in turn contains data and metadata for one overstort each.\n",
    "    \"\"\"\n",
    "    # Read the xml file and extract the tree and root. Then filter to get the series branch.\n",
    "    tree = etree.parse(path_xml)\n",
    "    root = tree.getroot()\n",
    "    list_of_series = [child for child in root.getchildren() if 'series' in child.tag]\n",
    "\n",
    "    return list_of_series\n",
    "\n",
    "def series_to_dataframe(serie):\n",
    "    \"\"\"Takes an individual Serie output by the function \"parse_xml\", transforms the data and metadata, \n",
    "    creates a clear dataframe.\n",
    "    \"\"\"\n",
    "    # Extract the ID from the metadata. Used as column header in output.\n",
    "    header = serie.find('{http://www.wldelft.nl/fews/PI}header')\n",
    "    id = header.find('{http://www.wldelft.nl/fews/PI}stationName').text\n",
    "    \n",
    "    # Extract the actual data and reformat into a dataframe wtih a datetime index. \n",
    "    events = serie.findall('{http://www.wldelft.nl/fews/PI}event')\n",
    "    data = pd.DataFrame([i.values() for i in events])\n",
    "    index = pd.DatetimeIndex(pd.to_datetime(data[0]+' '+data[1], format=\"%Y-%m-%d %H:%M:%S\"))\n",
    "    data_indexed = pd.DataFrame({id: data[2].values}, index=index)\n",
    "    data_divided = pd.to_numeric(data_indexed[id]).div(3600)\n",
    "    \n",
    "    return data_divided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop over all xml files and generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'hist':\n",
    "    path_input_overstorten_hist = join(path_data,r'input_afvoeren\\timeseries\\overstorten\\hist')\n",
    "    input_files = glob.glob(path_input_overstorten_hist + '/*.xml')\n",
    "    data_hist_ls = []\n",
    "\n",
    "    # I suggest to view one of the input xml files with this code, to understand the tree and its branches.\n",
    "    for path_xml in input_files:\n",
    "        \n",
    "        ls_data_year = []\n",
    "        list_of_series = parse_xml(path_xml)\n",
    "\n",
    "        for serie in list_of_series:\n",
    "            dataframe = series_to_dataframe(serie)\n",
    "            ls_data_year.append(dataframe)\n",
    "\n",
    "        # Combine outputs series vertically, merging on the datetime index. Then add the output of this year to the list of outputs from all years.\n",
    "        df_data_year = pd.concat(ls_data_year,axis='columns',verify_integrity=True)\n",
    "        data_hist_ls.append(df_data_year)\n",
    "\n",
    "    # Combine outputs from different years horizontally\n",
    "    output_overstorten_hist = pd.concat(data_hist_ls,axis='index',verify_integrity=True)\n",
    "    output_overstorten = output_overstorten_hist.fillna(0)\n",
    "\n",
    "    if SAVE_INTERMEDIATE_RESULTS == True:\n",
    "        output_overstorten.to_csv(join(path_data, r'output_afvoeren\\timeseries\\overstorten', 'hist.csv'), sep = ';')\n",
    "        \n",
    "elif 'ghg' in SCENARIO:\n",
    "    path_input_overstorten_blokbuien = join(path_data,r'input_afvoeren\\timeseries\\overstorten\\blokbuien')\n",
    "    input_files = glob.glob(path_input_overstorten_blokbuien + '/*.xlsx')\n",
    "\n",
    "    for file in input_files:\n",
    "        excel_table = pd.read_excel(file, skiprows=3)\n",
    "        index_col = excel_table['code OS']\n",
    "        data = excel_table.filter(regex='Unnamed', axis='columns').div(3600) # The columns containing the data are prefixed by 'Unnamed' due to Excel structure. \n",
    "        data.columns = range(1,76)\n",
    "        data.index = index_col\n",
    "        data_transposed = data.T\n",
    "\n",
    "        output_overstorten = data_transposed\n",
    "        \n",
    "        if SAVE_INTERMEDIATE_RESULTS == True:\n",
    "            output_overstorten.to_csv(join(path_data, r'output_afvoeren\\timeseries\\overstorten', basename(splitext(file)[0])+ '.csv'), sep = ';')\n",
    "else:\n",
    "    print('No valid scenario given as input.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine data into final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_combined = pd.concat([output_landelijkeafvoer, output_overstorten], keys=['Defined Lateral Disc', 'Lateral disch.(m3/s)'], axis=1)\n",
    "output_combined = output_combined[pd.to_datetime(\"2010-11-09 00:00:00\", format='%Y-%m-%d %H:%M:%S'):pd.to_datetime(\"2010-11-21 23:00:00\", format='%Y-%m-%d %H:%M:%S')]\n",
    "output_final = output_combined.fillna(0)\n",
    "output_final.to_csv(join(path_data, r'output_afvoeren\\timeseries', 'combinedoutput.csv'), sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_landelijkeafvoer.columns)\n",
    "len(output_overstorten.columns)\n",
    "len(output_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "328471a73e0747a21d520fbf062764676704a0ceaa1aebddbe8f003201a80ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
